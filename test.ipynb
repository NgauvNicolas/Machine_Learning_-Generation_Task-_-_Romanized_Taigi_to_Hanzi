{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d91ed720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Chargement et préparation des données pour Keras (Seq2Seq avec Attention) --\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"Noto Sans CJK JP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c7862c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Nettoyage et normalisation ---\n",
    "def clean_romanized(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def clean_sinogram(text):\n",
    "    punctuations = r\"[，。！？、：；「」『』《》〈〉（）(){}【】\\[\\]\\\"\\'“”‘’.,!?;:…\\-—~·•◦→←«»]\"\n",
    "    text = re.sub(punctuations, \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \"\", text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "238152bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Chargement du corpus ---\n",
    "def load_data(path):\n",
    "    data = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            obj = json.loads(line.strip())\n",
    "            if \"r\" in obj and \"j\" in obj:\n",
    "                romanized = clean_romanized(obj[\"r\"])\n",
    "                sinograms = clean_sinogram(obj[\"j\"])\n",
    "                if romanized and sinograms:\n",
    "                    data.append((romanized, sinograms))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ba77594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Tokenisation ---\n",
    "def tokenize_romanized(s):\n",
    "    return s.strip().split(\" \")\n",
    "\n",
    "def tokenize_sinogrammes(s):\n",
    "    return list(s.strip())\n",
    "\n",
    "# --- Construction des vocabulaires ---\n",
    "def build_vocab(sequences, special_tokens=[\"<PAD>\", \"<BOS>\", \"<EOS>\", \"<UNK>\"]):\n",
    "    vocab = set(token for seq in sequences for token in seq)\n",
    "    vocab = special_tokens + sorted(vocab)\n",
    "    token_to_id = {tok: idx for idx, tok in enumerate(vocab)}\n",
    "    id_to_token = {idx: tok for tok, idx in token_to_id.items()}\n",
    "    return token_to_id, id_to_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "90eed1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Conversion en indices ---\n",
    "def convert_to_ids(sequences, token_to_id, bos=False, eos=False):\n",
    "    out = []\n",
    "    for seq in sequences:\n",
    "        ids = []\n",
    "        if bos:\n",
    "            ids.append(token_to_id[\"<BOS>\"])\n",
    "        for token in seq:\n",
    "            ids.append(token_to_id.get(token, token_to_id[\"<UNK>\"]))\n",
    "        if eos:\n",
    "            ids.append(token_to_id[\"<EOS>\"])\n",
    "        out.append(ids)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e2be550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Pipeline complet ---\n",
    "def prepare_data(jsonl_path, maxlen_r=30, maxlen_j=30, test_size=0.2):\n",
    "    data = load_data(jsonl_path)\n",
    "\n",
    "    romanized_seqs = [tokenize_romanized(r) for r, _ in data]\n",
    "    sinogram_seqs = [tokenize_sinogrammes(j) for _, j in data]\n",
    "\n",
    "    tok2id_r, id2tok_r = build_vocab(romanized_seqs)\n",
    "    tok2id_j, id2tok_j = build_vocab(sinogram_seqs)\n",
    "\n",
    "    X = convert_to_ids(romanized_seqs, tok2id_r)\n",
    "    y = convert_to_ids(sinogram_seqs, tok2id_j, bos=True, eos=True)\n",
    "\n",
    "    X_pad = pad_sequences(X, maxlen=maxlen_r, padding=\"post\", truncating=\"post\", value=tok2id_r[\"<PAD>\"])\n",
    "    y_pad = pad_sequences(y, maxlen=maxlen_j, padding=\"post\", truncating=\"post\", value=tok2id_j[\"<PAD>\"])\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_pad, y_pad, test_size=test_size, random_state=42)\n",
    "\n",
    "    return {\n",
    "        \"X_train\": X_train, \"X_test\": X_test,\n",
    "        \"y_train\": y_train, \"y_test\": y_test,\n",
    "        \"tok2id_r\": tok2id_r, \"id2tok_r\": id2tok_r,\n",
    "        \"tok2id_j\": tok2id_j, \"id2tok_j\": id2tok_j\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82118185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger et préparer les données\n",
    "data = prepare_data(\"taigi_ime_data.jsonl\")\n",
    "\n",
    "# --- Séparation des séquences cible ---\n",
    "def split_decoder_inputs_outputs(y, pad_token=0):\n",
    "    decoder_input = y[:, :-1]\n",
    "    decoder_output = y[:, 1:]\n",
    "    return decoder_input, decoder_output\n",
    "\n",
    "decoder_input_train, decoder_target_train = split_decoder_inputs_outputs(data[\"y_train\"])\n",
    "decoder_input_test, decoder_target_test = split_decoder_inputs_outputs(data[\"y_test\"])\n",
    "\n",
    "# --- Modèle Seq2Seq avec Attention ---\n",
    "def build_seq2seq_attention_model(input_vocab_size, target_vocab_size, embedding_dim=128, encoder_units=256, decoder_units=256, maxlen_input=30, maxlen_target=30):\n",
    "    encoder_inputs = keras.Input(shape=(maxlen_input,), name=\"encoder_inputs\")\n",
    "    enc_emb = layers.Embedding(input_vocab_size, embedding_dim)(encoder_inputs)\n",
    "    encoder_lstm = layers.LSTM(encoder_units, return_sequences=True, return_state=True)\n",
    "    encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
    "\n",
    "    decoder_inputs = keras.Input(shape=(maxlen_target,), name=\"decoder_inputs\")\n",
    "    dec_emb = layers.Embedding(target_vocab_size, embedding_dim)(decoder_inputs)\n",
    "    decoder_lstm = layers.LSTM(decoder_units, return_sequences=True, return_state=True)\n",
    "    decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=[state_h, state_c])\n",
    "\n",
    "    attention_layer = layers.AdditiveAttention(name=\"attention\")\n",
    "    attention_output = attention_layer([decoder_outputs, encoder_outputs])\n",
    "    concat_attention = layers.Concatenate(axis=-1)([decoder_outputs, attention_output])\n",
    "    output = layers.TimeDistributed(layers.Dense(target_vocab_size, activation=\"softmax\"))(concat_attention)\n",
    "\n",
    "    model = keras.Model([encoder_inputs, decoder_inputs], output)\n",
    "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e1e0cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramètres et modèle\n",
    "input_vocab_size = len(data[\"tok2id_r\"])\n",
    "target_vocab_size = len(data[\"tok2id_j\"])\n",
    "maxlen_input = data[\"X_train\"].shape[1]\n",
    "maxlen_target = data[\"y_train\"].shape[1] - 1\n",
    "\n",
    "model = build_seq2seq_attention_model(\n",
    "    input_vocab_size, target_vocab_size,\n",
    "    maxlen_input=maxlen_input,\n",
    "    maxlen_target=maxlen_target\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d2734e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m1619/1619\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m963s\u001b[0m 592ms/step - accuracy: 0.6026 - loss: 2.7889 - val_accuracy: 0.6631 - val_loss: 2.0567\n",
      "Epoch 2/20\n",
      "\u001b[1m1619/1619\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m680s\u001b[0m 406ms/step - accuracy: 0.6855 - loss: 1.8663 - val_accuracy: 0.7549 - val_loss: 1.3573\n",
      "Epoch 3/20\n",
      "\u001b[1m1619/1619\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m634s\u001b[0m 391ms/step - accuracy: 0.7923 - loss: 1.1023 - val_accuracy: 0.8620 - val_loss: 0.7269\n",
      "Epoch 4/20\n",
      "\u001b[1m1619/1619\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m611s\u001b[0m 378ms/step - accuracy: 0.8971 - loss: 0.5076 - val_accuracy: 0.9119 - val_loss: 0.4530\n",
      "Epoch 5/20\n",
      "\u001b[1m1619/1619\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m611s\u001b[0m 378ms/step - accuracy: 0.9446 - loss: 0.2612 - val_accuracy: 0.9323 - val_loss: 0.3437\n",
      "Epoch 6/20\n",
      "\u001b[1m1619/1619\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m597s\u001b[0m 369ms/step - accuracy: 0.9642 - loss: 0.1617 - val_accuracy: 0.9468 - val_loss: 0.2807\n",
      "Epoch 7/20\n",
      "\u001b[1m1619/1619\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m602s\u001b[0m 372ms/step - accuracy: 0.9745 - loss: 0.1115 - val_accuracy: 0.9527 - val_loss: 0.2545\n",
      "Epoch 8/20\n",
      "\u001b[1m1619/1619\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m610s\u001b[0m 377ms/step - accuracy: 0.9810 - loss: 0.0815 - val_accuracy: 0.9581 - val_loss: 0.2355\n",
      "Epoch 9/20\n",
      "\u001b[1m1619/1619\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m590s\u001b[0m 364ms/step - accuracy: 0.9850 - loss: 0.0643 - val_accuracy: 0.9617 - val_loss: 0.2212\n",
      "Epoch 10/20\n",
      "\u001b[1m1619/1619\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m620s\u001b[0m 383ms/step - accuracy: 0.9877 - loss: 0.0521 - val_accuracy: 0.9630 - val_loss: 0.2172\n",
      "Epoch 11/20\n",
      "\u001b[1m1619/1619\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m589s\u001b[0m 364ms/step - accuracy: 0.9898 - loss: 0.0432 - val_accuracy: 0.9652 - val_loss: 0.2122\n",
      "Epoch 12/20\n",
      "\u001b[1m1619/1619\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m590s\u001b[0m 364ms/step - accuracy: 0.9912 - loss: 0.0373 - val_accuracy: 0.9671 - val_loss: 0.2048\n",
      "Epoch 13/20\n",
      "\u001b[1m1619/1619\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m590s\u001b[0m 365ms/step - accuracy: 0.9925 - loss: 0.0318 - val_accuracy: 0.9683 - val_loss: 0.2022\n",
      "Epoch 14/20\n",
      "\u001b[1m1619/1619\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m590s\u001b[0m 364ms/step - accuracy: 0.9932 - loss: 0.0282 - val_accuracy: 0.9688 - val_loss: 0.2036\n",
      "Epoch 15/20\n",
      "\u001b[1m1619/1619\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m590s\u001b[0m 364ms/step - accuracy: 0.9938 - loss: 0.0260 - val_accuracy: 0.9700 - val_loss: 0.2018\n",
      "Epoch 16/20\n",
      "\u001b[1m1619/1619\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m590s\u001b[0m 365ms/step - accuracy: 0.9942 - loss: 0.0239 - val_accuracy: 0.9705 - val_loss: 0.2009\n",
      "Epoch 17/20\n",
      "\u001b[1m1619/1619\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m590s\u001b[0m 365ms/step - accuracy: 0.9950 - loss: 0.0211 - val_accuracy: 0.9705 - val_loss: 0.2060\n",
      "Epoch 18/20\n",
      "\u001b[1m1619/1619\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m591s\u001b[0m 365ms/step - accuracy: 0.9953 - loss: 0.0200 - val_accuracy: 0.9713 - val_loss: 0.2014\n",
      "Epoch 19/20\n",
      "\u001b[1m1619/1619\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m591s\u001b[0m 365ms/step - accuracy: 0.9958 - loss: 0.0180 - val_accuracy: 0.9721 - val_loss: 0.1995\n",
      "Epoch 20/20\n",
      "\u001b[1m1619/1619\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m593s\u001b[0m 366ms/step - accuracy: 0.9961 - loss: 0.0169 - val_accuracy: 0.9719 - val_loss: 0.2029\n"
     ]
    }
   ],
   "source": [
    "# Entraînement\n",
    "early_stop = EarlyStopping(patience=3, restore_best_weights=True)\n",
    "history = model.fit(\n",
    "    [data[\"X_train\"], decoder_input_train],\n",
    "    decoder_target_train,\n",
    "    validation_split=0.2,\n",
    "    batch_size=32,\n",
    "    epochs=20,\n",
    "    callbacks=[early_stop]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e222ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_seq2seq_learning_curves(history):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Sous-plot 1 : loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history[\"loss\"], label=\"train\")\n",
    "    plt.plot(history.history[\"val_loss\"], label=\"val\")\n",
    "    plt.title(\"Seq2Seq - Loss au fil des époques\")\n",
    "    plt.xlabel(\"Époques\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Sous-plot 2 : accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history[\"accuracy\"], label=\"train\")\n",
    "    plt.plot(history.history[\"val_accuracy\"], label=\"val\")\n",
    "    plt.title(\"Seq2Seq - Accuracy (caractère) au fil des époques\")\n",
    "    plt.xlabel(\"Époques\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "40d4728d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "too many positional arguments",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m encoder_embedding_layer \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      4\u001b[0m encoder_lstm \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m----> 5\u001b[0m enc_emb \u001b[38;5;241m=\u001b[39m encoder_embedding_layer(encoder_inputs)\n\u001b[1;32m      6\u001b[0m encoder_outputs, state_h_enc, state_c_enc \u001b[38;5;241m=\u001b[39m encoder_lstm(enc_emb)\n\u001b[1;32m      7\u001b[0m encoder_model \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mModel(encoder_inputs, [encoder_outputs, state_h_enc, state_c_enc])\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/inspect.py:3259\u001b[0m, in \u001b[0;36mSignature.bind\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3254\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m/\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   3255\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get a BoundArguments object, that maps the passed `args`\u001b[39;00m\n\u001b[1;32m   3256\u001b[0m \u001b[38;5;124;03m    and `kwargs` to the function's signature.  Raises `TypeError`\u001b[39;00m\n\u001b[1;32m   3257\u001b[0m \u001b[38;5;124;03m    if the passed arguments can not be bound.\u001b[39;00m\n\u001b[1;32m   3258\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bind(args, kwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/inspect.py:3180\u001b[0m, in \u001b[0;36mSignature._bind\u001b[0;34m(self, args, kwargs, partial)\u001b[0m\n\u001b[1;32m   3178\u001b[0m     param \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(parameters)\n\u001b[1;32m   3179\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m-> 3180\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoo many positional arguments\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3181\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m (_VAR_KEYWORD, _KEYWORD_ONLY):\n\u001b[1;32m   3183\u001b[0m         \u001b[38;5;66;03m# Looks like we have no parameter for this positional\u001b[39;00m\n\u001b[1;32m   3184\u001b[0m         \u001b[38;5;66;03m# argument\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: too many positional arguments"
     ]
    }
   ],
   "source": [
    "# --- Création des modèles encoder / decoder pour l'inférence ---\n",
    "encoder_inputs = model.input[0]\n",
    "encoder_embedding_layer = model.layers[1]\n",
    "encoder_lstm = model.layers[2]\n",
    "enc_emb = encoder_embedding_layer(encoder_inputs)\n",
    "encoder_outputs, state_h_enc, state_c_enc = encoder_lstm(enc_emb)\n",
    "encoder_model = keras.Model(encoder_inputs, [encoder_outputs, state_h_enc, state_c_enc])\n",
    "\n",
    "decoder_inputs = model.input[1]\n",
    "decoder_embedding_layer = model.layers[4]\n",
    "decoder_lstm = model.layers[5]\n",
    "attention_layer = model.get_layer(\"attention\")\n",
    "concat_layer = model.layers[7]\n",
    "dense_layer = model.layers[8]\n",
    "\n",
    "decoder_input_single = keras.Input(shape=(1,), name=\"decoder_input_t\")\n",
    "enc_outputs_input = keras.Input(shape=(None, encoder_outputs.shape[-1]), name=\"encoder_outputs\")\n",
    "state_h_input = keras.Input(shape=(decoder_lstm.units,), name=\"h_input\")\n",
    "state_c_input = keras.Input(shape=(decoder_lstm.units,), name=\"c_input\")\n",
    "\n",
    "dec_emb = decoder_embedding_layer(decoder_input_single)\n",
    "dec_out, state_h_new, state_c_new = decoder_lstm(dec_emb, initial_state=[state_h_input, state_c_input])\n",
    "attn_out = attention_layer([dec_out, enc_outputs_input])\n",
    "concat_out = concat_layer([dec_out, attn_out])\n",
    "final_output = dense_layer(concat_out)\n",
    "\n",
    "decoder_model = keras.Model(\n",
    "    inputs=[decoder_input_single, enc_outputs_input, state_h_input, state_c_input],\n",
    "    outputs=[final_output, state_h_new, state_c_new]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44a54b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Décodage greedy avec attention ---\n",
    "def decode_sequence_infer(input_seq, max_len=30):\n",
    "    enc_outs, h, c = encoder_model.predict(input_seq)\n",
    "    tok2id_j = data[\"tok2id_j\"]\n",
    "    id2tok_j = data[\"id2tok_j\"]\n",
    "    BOS, EOS = tok2id_j[\"<BOS>\"], tok2id_j[\"<EOS>\"]\n",
    "    target_seq = np.array([[BOS]])\n",
    "    decoded_tokens = []\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        output, h, c = decoder_model.predict([target_seq, enc_outs, h, c])\n",
    "        token_id = np.argmax(output[0, -1, :])\n",
    "        if token_id == EOS:\n",
    "            break\n",
    "        decoded_tokens.append(id2tok_j.get(token_id, \"<UNK>\"))\n",
    "        target_seq = np.array([[token_id]])\n",
    "\n",
    "    return \"\".join(decoded_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263cba0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Affichage sur un exemple ---\n",
    "def infer_with_encoder_decoder(example_idx):\n",
    "    x_input = data[\"X_test\"][example_idx:example_idx+1]\n",
    "    gold_ids = data[\"y_test\"][example_idx]\n",
    "    pad_id = data[\"tok2id_j\"][\"<PAD>\"]\n",
    "    r_tok = [data[\"id2tok_r\"][i] for i in x_input[0] if i != data[\"tok2id_r\"][\"<PAD>\"]]\n",
    "    j_gold = [data[\"id2tok_j\"][i] for i in gold_ids if i not in (pad_id, data[\"tok2id_j\"][\"<BOS>\"], data[\"tok2id_j\"][\"<EOS>\"])]\n",
    "    j_pred = decode_sequence_infer(x_input)\n",
    "    print(\"🔤 Romanisé :\", \" \".join(r_tok))\n",
    "    print(\"✅ Réel     :\", \"\".join(j_gold))\n",
    "    print(\"🤖 Prédit   :\", j_pred)\n",
    "\n",
    "# Exemple\n",
    "infer_with_encoder_decoder(0)\n",
    "infer_with_encoder_decoder(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b09e792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Évaluation globale : CER (Character Error Rate) et BLEU ---\n",
    "from jiwer import cer\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "\n",
    "def evaluate_model_on_test(data, max_examples=200):\n",
    "    pad_id = data[\"tok2id_j\"][\"<PAD>\"]\n",
    "    bos_id = data[\"tok2id_j\"][\"<BOS>\"]\n",
    "    eos_id = data[\"tok2id_j\"][\"<EOS>\"]\n",
    "    id2tok_j = data[\"id2tok_j\"]\n",
    "\n",
    "    cer_scores = []\n",
    "    bleu_scores = []\n",
    "    smoothie = SmoothingFunction().method4\n",
    "\n",
    "    for i in range(min(max_examples, len(data[\"X_test\"]))):\n",
    "        x_input = data[\"X_test\"][i:i+1]\n",
    "        gold_ids = data[\"y_test\"][i]\n",
    "        gold = [id2tok_j[tok] for tok in gold_ids if tok not in (pad_id, bos_id, eos_id)]\n",
    "        pred = list(decode_sequence_infer(x_input))\n",
    "\n",
    "        ref_str = \"\".join(gold)\n",
    "        hyp_str = \"\".join(pred)\n",
    "\n",
    "        cer_scores.append(cer(ref_str, hyp_str))\n",
    "        bleu_scores.append(sentence_bleu([gold], pred, smoothing_function=smoothie))\n",
    "\n",
    "    avg_cer = np.mean(cer_scores)\n",
    "    avg_bleu = np.mean(bleu_scores)\n",
    "\n",
    "    print(f\"📏 CER moyen  : {avg_cer:.4f} ({avg_cer * 100:.2f}%)\")\n",
    "    print(f\"🟦 BLEU moyen : {avg_bleu:.4f}\")\n",
    "    return avg_cer, avg_bleu\n",
    "\n",
    "# Évaluation\n",
    "evaluate_model_on_test(data, max_examples=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a8bc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Phase 7 : Baseline simple par lookup mot-à-mot ---\n",
    "def build_lookup_table(data):\n",
    "    table = {}\n",
    "    for x_seq, y_seq in zip(data[\"X_train\"], data[\"y_train\"]):\n",
    "        for x_id, y_id in zip(x_seq, y_seq[1:]):  # y_seq[1:] pour ignorer BOS\n",
    "            if x_id == data[\"tok2id_r\"][\"<PAD>\"] or y_id in (\n",
    "                data[\"tok2id_j\"][\"<PAD>\"], data[\"tok2id_j\"][\"<EOS>\"]):\n",
    "                continue\n",
    "            if x_id not in table:\n",
    "                table[x_id] = Counter()\n",
    "            table[x_id][y_id] += 1\n",
    "    return {x: counter.most_common(1)[0][0] for x, counter in table.items()}\n",
    "\n",
    "\n",
    "def lookup_baseline_predict(x_seq, lookup_table, unk_id):\n",
    "    return [lookup_table.get(tok, unk_id) for tok in x_seq if tok != 0]\n",
    "\n",
    "\n",
    "def evaluate_lookup_baseline(data, max_examples=200):\n",
    "    id2tok_j = data[\"id2tok_j\"]\n",
    "    pad_id = data[\"tok2id_j\"][\"<PAD>\"]\n",
    "    bos_id = data[\"tok2id_j\"][\"<BOS>\"]\n",
    "    eos_id = data[\"tok2id_j\"][\"<EOS>\"]\n",
    "    unk_id = data[\"tok2id_j\"][\"<UNK>\"]\n",
    "\n",
    "    table = build_lookup_table(data)\n",
    "\n",
    "    cer_scores = []\n",
    "    bleu_scores = []\n",
    "    smoothie = SmoothingFunction().method4\n",
    "\n",
    "    for i in range(min(max_examples, len(data[\"X_test\"]))):\n",
    "        x_seq = data[\"X_test\"][i]\n",
    "        y_gold_ids = data[\"y_test\"][i]\n",
    "\n",
    "        gold = [id2tok_j[i] for i in y_gold_ids if i not in (pad_id, bos_id, eos_id)]\n",
    "        pred_ids = lookup_baseline_predict(x_seq, table, unk_id)\n",
    "        pred = [id2tok_j.get(i, \"<UNK>\") for i in pred_ids]\n",
    "\n",
    "        cer_scores.append(cer(\"\".join(gold), \"\".join(pred)))\n",
    "        bleu_scores.append(sentence_bleu([gold], pred, smoothing_function=smoothie))\n",
    "\n",
    "    avg_cer = np.mean(cer_scores)\n",
    "    avg_bleu = np.mean(bleu_scores)\n",
    "\n",
    "    print(f\"🔁 Baseline CER  : {avg_cer:.4f} ({avg_cer * 100:.2f}%)\")\n",
    "    print(f\"🔁 Baseline BLEU : {avg_bleu:.4f}\")\n",
    "    return avg_cer, avg_bleu\n",
    "\n",
    "# Évaluer la baseline\n",
    "evaluate_lookup_baseline(data, max_examples=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0800c61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Phase 8 : Modèle autoregressif simplifié (LSTM unidirectionnel) ---\n",
    "def build_autoregressive_model(vocab_size, embedding_dim=128, rnn_units=256, maxlen_input=30):\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=(maxlen_input,), name=\"input_seq\"),\n",
    "        layers.Embedding(vocab_size, embedding_dim),\n",
    "        layers.LSTM(rnn_units, return_sequences=True),\n",
    "        layers.TimeDistributed(layers.Dense(vocab_size, activation=\"softmax\"))\n",
    "    ])\n",
    "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "# Préparation des données autoregressives (X = sinogrammes[:-1], y = sinogrammes[1:])\n",
    "def prepare_autoregressive_data(data):\n",
    "    Y = data[\"y_train\"]\n",
    "    X_auto = Y[:, :-1]\n",
    "    y_auto = Y[:, 1:]\n",
    "    return X_auto, y_auto\n",
    "\n",
    "X_auto, y_auto = prepare_autoregressive_data(data)\n",
    "\n",
    "# Modèle\n",
    "vocab_size_j = len(data[\"tok2id_j\"])\n",
    "auto_model = build_autoregressive_model(vocab_size=vocab_size_j, maxlen_input=X_auto.shape[1])\n",
    "\n",
    "# Entraînement\n",
    "history_auto = auto_model.fit(\n",
    "    X_auto, y_auto,\n",
    "    validation_split=0.2,\n",
    "    batch_size=32,\n",
    "    epochs=10,\n",
    "    callbacks=[EarlyStopping(patience=2, restore_best_weights=True)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb711c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualisation des courbes d'apprentissage pour le modèle autoregressif ---\n",
    "def plot_autoregressive_learning_curves(history):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Sous-plot 1 : loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history[\"loss\"], label=\"train\")\n",
    "    plt.plot(history.history[\"val_loss\"], label=\"val\")\n",
    "    plt.title(\"Autoregressif - Loss au fil des époques\")\n",
    "    plt.xlabel(\"Époques\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Sous-plot 2 : accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history[\"accuracy\"], label=\"train\")\n",
    "    plt.plot(history.history[\"val_accuracy\"], label=\"val\")\n",
    "    plt.title(\"Autoregressif - Accuracy au fil des époques\")\n",
    "    plt.xlabel(\"Époques\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Tracer les courbes d'entraînement\n",
    "plot_autoregressive_learning_curves(history_auto)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33405871",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Exemple d'inférence (auto-génération à partir du BOS)\n",
    "def generate_from_bos(model, tok2id, id2tok, maxlen=30):\n",
    "    BOS = tok2id[\"<BOS>\"]\n",
    "    EOS = tok2id[\"<EOS>\"]\n",
    "    PAD = tok2id[\"<PAD>\"]\n",
    "\n",
    "    input_seq = [BOS]\n",
    "    for _ in range(maxlen):\n",
    "        padded = pad_sequences([input_seq], maxlen=maxlen, padding=\"post\", value=PAD)\n",
    "        preds = model.predict(padded, verbose=0)\n",
    "        next_id = np.argmax(preds[0, len(input_seq)-1])\n",
    "        if next_id == EOS:\n",
    "            break\n",
    "        input_seq.append(next_id)\n",
    "\n",
    "    return \"\".join(id2tok[i] for i in input_seq[1:] if i != PAD)\n",
    "\n",
    "# Exemple\n",
    "print(\"🌀 Exemple génération autoregressive:\")\n",
    "print(generate_from_bos(auto_model, data[\"tok2id_j\"], data[\"id2tok_j\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b85b6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparative_learning_curves(history_seq2seq, history_auto):\n",
    "    plt.figure(figsize=(14, 5))\n",
    "\n",
    "    # === Perte (loss)\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history_seq2seq.history[\"loss\"], label=\"Seq2Seq - train\", color=\"blue\")\n",
    "    plt.plot(history_seq2seq.history[\"val_loss\"], label=\"Seq2Seq - val\", color=\"blue\", linestyle=\"--\")\n",
    "    plt.plot(history_auto.history[\"loss\"], label=\"AutoReg - train\", color=\"green\")\n",
    "    plt.plot(history_auto.history[\"val_loss\"], label=\"AutoReg - val\", color=\"green\", linestyle=\"--\")\n",
    "    plt.title(\"Courbes de perte (Loss)\")\n",
    "    plt.xlabel(\"Époques\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # === Précision (accuracy)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history_seq2seq.history[\"accuracy\"], label=\"Seq2Seq - train\", color=\"blue\")\n",
    "    plt.plot(history_seq2seq.history[\"val_accuracy\"], label=\"Seq2Seq - val\", color=\"blue\", linestyle=\"--\")\n",
    "    plt.plot(history_auto.history[\"accuracy\"], label=\"AutoReg - train\", color=\"green\")\n",
    "    plt.plot(history_auto.history[\"val_accuracy\"], label=\"AutoReg - val\", color=\"green\", linestyle=\"--\")\n",
    "    plt.title(\"Accuracy caractère par caractère\")\n",
    "    plt.xlabel(\"Époques\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
